Hi! I am Yafu Li, a researcher at Shanghai AI Lab, supervised by [Prof. Yu Cheng](https://ych133.github.io/). I earned my PhD through joint training at Zhejiang University and Westlake University under the guidance of [Prof. Yue Zhang](https://frcchang.github.io/). 

My research focuses on test-time scaling, trustworthy AI and machine translation.

:sparkles: We are looking for interns and joint PhD candidates (with THU, PKU, SJTU, FDU, etc.) to work on cutting-edge research in large-scale models. Our focus areas include zero reinforcement learning (e.g., R1-zero), test-time scaling, and trustworthy AI. If you are interested, please feel free to contact me at yafuly@gmail.com.

 ## Test-time Scaling

- [A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond](https://arxiv.org/abs/2503.21614)
- [Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback](https://arxiv.org/abs/2501.12895)
- [From Drafts to Answers: Unlocking LLM Potential via Aggregation Fine-Tuning](https://arxiv.org/abs/2501.11877)
- [Multi-LLM Collaborative Search for Complex Problem Solving](https://arxiv.org/abs/2502.18873)

## Trustworthy AI

- [MAGE: Machine-generated Text Detection in the Wild](https://aclanthology.org/2024.acl-long.3/#)
- [Spotting AIâ€™s Touch: Identifying LLM-Paraphrased Spans in Text](https://aclanthology.org/2024.findings-acl.423/)
- [Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/abs/2309.01219)

## Machine Translation

- [Lost in Literalism: How Supervised Training Shapes Translationese in LLMs](https://arxiv.org/abs/2503.04369)
- [Explicit Syntactic Guidance for Neural Text Generation](https://aclanthology.org/2023.acl-long.788/)
- [Multi-Granularity Optimization for Non-Autoregressive Translation](https://aclanthology.org/2022.emnlp-main.339/)
