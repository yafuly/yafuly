<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yafu Li</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yafu Li (ÊùéÈõÖÂ§´)</name>
              </p>
              <p>I am a researcher in Shanghai / Pujiang AI Lab, under the supervision of <a href="https://ych133.github.io/">Prof. Yu Cheng</a>. I earned my PhD through joint training at Zhejiang University and Westlake University under the guidance of <a href="https://frcchang.github.io/">Prof. Yue Zhang</a>. During my internship at Tencent AI Lab, I collaborated closely with <a href="https://nealcly.github.io/">Dr. Leyang Cui</a> and <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ">Dr. Wei Bi</a>.
              <br>
              <br>
              I earned my Bachelor‚Äôs degree at Wuhan University and subsequently pursued a Master‚Äôs degree at the University of Edinburgh, where I was supervised by <a href="https://homepages.inf.ed.ac.uk/alex/">Prof. Alex Lascarides</a>. Prior to my PhD, I worked as an NLP researcher in Noah Ark's lab at Huawei, under the mentorship of <a href="https://scholar.google.com/citations?user=PPDE-uIAAAAJ&hl=en">Dr. Liangyou Li</a> and <a href="https://liuquncn.github.io">Prof. Qun Liu</a>. 
              </p>
              <p style="text-align:center">
                <a href="mailto:yafuly@gmail.com">Email</a> &nbsp/&nbsp
                <!-- CV(<a href="data/cv_en.pdf">En</a>/<a href="data/cv_zh.pdf">‰∏≠Êñá</a>) &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=gEceD-sAAAAJ">Google Scholar</a> &nbsp/&nbsp
		      <a href="https://www.semanticscholar.org/author/Yafu-Li/2110450452">Semantic Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/yafuly">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/yafuly">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JonBarron.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/self2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Open Positions</heading>
            <p>
              We are looking for <strong>interns</strong> and <strong>joint PhD candidates</strong> (with THU, PKU, SJTU, FDU, etc.) to work on cutting-edge research in large language models. Our focus areas include zero reinforcement learning (e.g., R1-zero), test-time scaling, and trustworthy AI. If you are interested, please feel free to contact me at yafuly@gmail.com.
            </p>
          </td>
        </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
              <p>
                My research focuses on test-time scaling, trustworthy AI and machine translation. *: equal contributions. &#8224: project lead or corresponding author.
              </p>
            </td>
          </tr>
        </tbody></table>



        
        <!-- Inference Scaling Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Reasoning and Inference Scaling</heading>
            </td>
          </tr>
        </tbody></table>
        
        <!-- Papers under Inference Scaling -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/luffy.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2504.14945" id="conf_deepfake-detect">
                <papertitle>Learning to Reason under Off-Policy Guidance</papertitle>
              </a>
              <br>
              Jianhao Yan<sup>*</sup>, <strong>Yafu Li</strong><sup>*</sup><sup>&#8224</sup>, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, Yue Zhang
              <br>
              <em>preprint</em>
              <br>
              <a href="https://github.com/ElliottYan/LUFFY">Github</a>
              /
              <a href="https://arxiv.org/abs/2504.14945">Paper</a>
              <p></p>
              <p>A RL framework to boost reasoning performance using off-policy guidance.</p>
            </td>
          </tr>
          
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/efficient_lrm_survey.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2503.21614" id="conf_deepfake-detect">
                <papertitle>A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond</papertitle>
              </a>
              <br>
              Xiaoye Qu<sup>*</sup><sup>&#8224</sup>, <strong>Yafu Li</strong><sup>*</sup><sup>&#8224</sup>, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, Peng Li, Wei Wei, Jing Shao, Chaochao Lu, Yue Zhang, Xian-Sheng Hua, Bowen Zhou, Yu Cheng
              <br>
              <em>preprint</em>
              <br>
              <a href="https://github.com/XiaoYee/Awesome_Efficient_LRM_Reasoning">Github</a>
              /
              <a href="https://arxiv.org/abs/2503.21614">Paper</a>
              <p></p>
              <p>We survey recent efforts to improve reasoning efficiency in Large Reasoning Models, highlighting inefficiency patterns in long reasoning traces and discussing solutions across the model lifecycle from pretraining to inference.</p>
            </td>
          </tr>
          
        
          
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tpo.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2501.12895" id="conf_deepfake-detect">
                <papertitle>Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback</papertitle>
              </a>
              <br>
              <strong>Yafu Li</strong>,
              Xuyang Hu,
              Xiaoye Qu,
              Linjie Li,
              Yu Cheng
              <br>
              <em>ICML 2025</em>
              <br>
              <a href="https://github.com/yafuly/TPO">Github</a>
              /
              <a href="https://arxiv.org/abs/2501.12895">Paper</a>
              <p></p>
              <p>We present Test-Time Preference Optimization (TPO) that aligns LLMs during inference, surpassing strong baselines aligned during training.</p>
            </td>
          </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/aft.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2501.11877" id="conf_deepfake-detect">
                <papertitle>From Drafts to Answers: Unlocking LLM Potential via Aggregation Fine-Tuning</papertitle>
              </a>
              <br>
              <strong>Yafu Li</strong><sup>*</sup>,
              Zhilin Wang<sup>*</sup>,
              Tingchen Fu,
              Ganqu Cui,
              Sen Yang,
              Yu Cheng
              <br>
              <em>preprint</em>
              <br>
              <a href="https://github.com/Linzwcs/AFT">Github</a>
              /
              <a href="https://arxiv.org/abs/2501.11877">Paper</a>
              <p></p>
              <p>We present Aggregation Fine-Tuning (AFT) where the model learns to aggregate multiple drafts into a single answer.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mosa.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2502.18873" id="conf_deepfake-detect">
                <papertitle>Multi-LLM Collaborative Search for Complex Problem Solving</papertitle>
              </a>
              <br>
              Sen Yang, <strong>Yafu Li</strong>, Wai Lam, Yu Cheng
              <br>
              <em>preprint</em>
              <br>
              <!-- <a href="https://github.com/yafuly/TPO">Github</a>
              / -->
              <a href="https://arxiv.org/abs/2502.18873">Paper</a>
              <p></p>
              <p>We propose Mixture-of-Search-Agents (MoSA), a test-time framework that coordinates multiple LLMs via Monte Carlo Tree Search to improve complex reasoning by aggregating diverse search trajectories.</p>
            </td>
          </tr>

        <!-- Trustworthy AI Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Trustworthy AI</heading>
            </td>
          </tr>
        </tbody></table>
        
        <!-- Papers under Trustworthy AI -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/deepfaketextdetect.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2305.13242" id="conf_deepfake-detect">
                <papertitle>MAGE: Machine-generated Text Detection in the Wild</papertitle>
              </a>
              <br>
              <strong>Yafu Li</strong>,
              Qintong Li,
              Leyang Cui,
              Wei Bi,
              Longyue Wang,
              Linyi Yang,
              Shuming Shi,
              Yue Zhang
              <br>
              <em>ACL</em>, 2024
              <br>
              <a href="https://github.com/yafuly/MAGE">Github</a>
              /
              <a href="https://arxiv.org/abs/2305.13242">Paper</a>
              <p></p>
              <p>We present a comprehensive benchmark dataset designed to assess the proficiency of machine-generated text detectors amidst real-world scenarios.</p>
            </td>
          </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ptd.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2405.12689" id="conf_ptd">
                <papertitle>Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text</papertitle>
              </a>
              <br>
              <strong>Yafu Li</strong><sup>*</sup>,
              Zhilin Wang<sup>*</sup>,
              Leyang Cui,
              Wei Bi,
              Shumin Shi,
              Yue Zhang
              <br>
              <em>ACL Findings</em>, 2024
              <br>
              <a href="https://github.com/Linzwcs/PASTED">Github</a>
              /
              <a href="https://arxiv.org/abs/2405.12689">Paper</a>
              <p></p>
              <p>We propose a novel task to identify "AI-touched" text spans in a fine-grained manner.</p>
            </td>
          </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/siren.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2309.01219" id="conf_siren">
                <papertitle>Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models</papertitle>
              </a>
              <br>
              Yue Zhang,
              <strong>Yafu Li</strong>,
              Leyang Cui,
              Deng Cai,
              Lemao Liu,
              Tingchen Fu,
              Xinting Huang,
              Enbo Zhao,
              Yu Zhang,
              Yulong Chen,
              Longyue Wang,
              Anh Tuan Luu,
              Wei Bi,
              Freda Shi,
              Shuming Shi
              <br>
              <em>preprint</em>
              <br>
              <a href="https://github.com/hillzhang1999/llm-hallucination-survey">Github</a>
              /
              <a href="https://arxiv.org/abs/2309.01219">Paper</a>
              <p></p>
              <p>A survey of hallucination in LLMs.</p>
            </td>
          </tr>
        </tbody></table>
        
        <!-- Machine Translation Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Machine Translation</heading>
            </td>
          </tr>
        </tbody></table>
        
        <!-- Papers under Machine Translation -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/llm_trans.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2503.04369" id="conf_syn-gen">
                <papertitle>Lost in Literalism: How Supervised Training Shapes Translationese in LLMs</papertitle>
              </a>
              <br>
              <strong>Yafu Li*</strong>,
              Ronghao Zhang*,
              Zhilin Wang,
              Huajiang Zhang,
              Leyang Cui,
              Yongjing Yin,
              Tong Xiao,
              Yue Zhang
              <br>
              <em>ACL</em>, 2025
              <br>
              <a href="https://github.com/yafuly/LLM_Translationese">Github</a>
              /
              <a href="https://arxiv.org/abs/2503.04369">Paper</a>
              <p></p>
              <p>A systematic study of the origin of translationese in LLMs and mitigation methods.</p>
            </td>    
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/syn_gen.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2306.11485" id="conf_syn-gen">
                <papertitle>Explicit Syntactic Guidance for Neural Text Generation</papertitle>
              </a>
              <br>
              <strong>Yafu Li</strong>,
              Leyang Cui,
              Jianhao Yan,
              Yongjing Yin,
              Wei Bi,
              Shuming Shi,
              Yue Zhang
              <br>
              <em>ACL</em>, 2023, <strong>Best Paper Nomination (1.6%)</strong>
              <br>
              <a href="https://github.com/yafuly/SyntaxGen">Github</a>
              /
              <a href="https://arxiv.org/abs/2306.11485">Paper</a>
              <p></p>
              <p>We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a top-down direction.</p>
            </td>    
          </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mgmo.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2210.11017" id="conf_mgmo">
                <papertitle>Multi-Granularity Optimization for Non-Autoregressive Translation</papertitle>
              </a>
              <br>
              <strong>Yafu Li</strong>,
              Leyang Cui,
              Yongjing Yin,
              Yue Zhang
              <br>
              <em>EMNLP</em>, 2022
              <br>
              <a href="https://github.com/yafuly/mgmo-nat">Github</a>
              /
              <a href="https://arxiv.org/abs/2210.11017">Paper</a>
              <p></p>
              <p>We propose multi-granularity optimization for non-autoregressive translation, which collects model behaviors on translation segments of various granularities and integrates feedback for backpropagation.</p>
            </td>    
          </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nat_vs_at.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2405.12788" id="conf_nat-vs-at">
                <papertitle>What Have We Achieved on Non-autoregressive Translation?</papertitle>
              </a>
              <br>
              <strong>Yafu Li</strong><sup>*</sup>,
              Huajian Zhang<sup>*</sup>,
              Jianhao Yan,
              Yongjing Yin,
              Yue Zhang
              <br>
              <em>ACL Findings</em>, 2024
              <br>
              <a href="https://github.com/HJZnlp/NAT_vs_AT">Github</a>
              /
              <a href="https://arxiv.org/abs/2405.12788">Paper</a>
              <p></p>
              <p>We present a systematic and comprehensive evaluation of NAT methods against AT.</p>
            </td>
          </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cg.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2105.14802v1" id="conf_lan">
                <papertitle>On Compositional Generalization of Neural Machine Translation</papertitle>
              </a>
              <br>
              <strong>Yafu Li</strong>,
              Yongjing Yin,
              Yulong Chen,
              Yue Zhang
              <br>
              <em>ACL</em>, 2021
              <br>
              <a href="https://github.com/yafuly/CoGnition">Github</a>
              /
              <a href="https://arxiv.org/abs/2105.14802v1">Paper</a>
              <p></p>
              <p>Neural machine translation suffers poor compositionality.</p>
            </td>    
          </tr>
        </tbody></table>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <heading>Education</heading>
      <p>
        PhD in Computer Science, <strong>Zhejiang University</strong> and <strong>Westlake University</strong> (2020.9-2024.9).
      </p>
      <p>
        Master of Science in Artificial Intelligence, <strong>University of Edinburgh</strong> (2017.9-2018.11).
      </p>
      <p>
        Bachelor of Engineering in Electronic Information Engineering, <strong>Wuhan University</strong> (2013.9-2017.6).
      </p>
    </td>
  </tr>
</tbody></table>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <heading>Experience</heading>
      <p>
        <strong>Research Intern</strong> at Tencent AI lab (2022.10-2024.5).
      </p>
      <p>
        <strong>Algorithem Engineer</strong> at Noah Ark's lab, Huawei (2018.12-2020.6).
      </p>
      <p>
        <strong>Software Engineering Intern</strong> at VMware, Beijing (2016.9-2017.5).
      </p>
    </td>
  </tr>
</tbody></table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <heading>Service</heading>
      <p>
        <strong>Area Chair:</strong> ACL 2025
      </p>
      <p>
        <strong>Conference Reviewer:</strong> ACL, EMNLP, COLING, ACL ARR, IJCAI.
      </p>
      <p>
        <strong>Journal Reviewer:</strong> TMLR, JAIR, TACL, TASLP, TBD, TALLIP.
      </p>
    </td>
  </tr>
</tbody></table>
  
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
    <heading>Honor</heading>
    <p>
      <strong>Outstanding Student Scholarship</strong> (Silver medal, Tencent Rhino-Bird Elite Program, 2024).
    </p>
    <p>
      <strong>National Scholarship</strong> (Ministry of Education, 2023).
    </p>
    <p>
      <strong>Dean's Medal</strong> (Westlake University, 2023).
    </p>
  </td>
</tr>
</tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:0px">
        <br>
        <p style="text-align:right;font-size:small;">
          Website's code is from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
          
        </p>
      </td>
    </tr>
  </tbody></table>
</td>
</tr>

</body>

</html>
